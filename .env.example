# cogniarch LLM Configuration
# Copy this file to .env and fill in your values:
#   cp .env.example .env
#
# Works with any OpenAI-compatible endpoint. Examples:
#
#   Anthropic:    https://api.anthropic.com/v1
#   OpenAI:       https://api.openai.com/v1
#   Ollama:       http://localhost:11434/v1  (no API key needed)
#   vLLM:         http://localhost:8000/v1   (no API key needed)
#   LM Studio:    http://localhost:1234/v1   (no API key needed)
#   Together AI:  https://api.together.xyz/v1
#   Groq:         https://api.groq.com/openai/v1
#   OpenRouter:   https://openrouter.ai/api/v1

AUTOCOG_LLM_BASE_URL=http://localhost:11434/v1
AUTOCOG_LLM_API_KEY=your-api-key-here
AUTOCOG_LLM_MODEL=opus
AUTOCOG_LLM_CHEAP_MODEL=sonnet
